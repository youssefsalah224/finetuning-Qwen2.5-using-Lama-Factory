{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXnC21zzEI1JGLvDI50T9v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youssefsalah224/finetuning-Qwen2.5-using-Lama-Factory/blob/main/Fine_Tuning_Qwen_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5M0QVHe9pUO",
        "outputId": "1c24f583-2922-4cef-cfcb-a5b216dec525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers==4.48.3 datasets==3.2.0 optimum==1.24.0\n",
        "!pip install -qU openai==1.61.0 wandb\n",
        "!pip install -qU json-repair==0.29.1\n",
        "!pip install -qU faker==35.2.0\n",
        "!pip install -qU vllm==0.7.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9if6Ybc92WC",
        "outputId": "85e6f353-4461-4996-c738-6584be10aa14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 427, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 239, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/resolvelib/resolvers.py\", line 229, in _get_updated_criteria\n",
            "    for requirement in self._p.get_dependencies(candidate=candidate):\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/provider.py\", line 247, in get_dependencies\n",
            "    return [r for r in candidate.iter_dependencies(with_requires) if r is not None]\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 401, in iter_dependencies\n",
            "    for r in self.dist.iter_dependencies():\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/importlib/_dists.py\", line 225, in iter_dependencies\n",
            "    elif not extras and req.marker.evaluate({\"extra\": \"\"}):\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/packaging/markers.py\", line 310, in evaluate\n",
            "    current_environment = cast(\"dict[str, str]\", default_environment())\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/typing.py\", line 2187, in cast\n",
            "    def cast(typ, val):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1586, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1684, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1700, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1762, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1028, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
            "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
            "    with self:\n",
            "         ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 865, in __exit__\n",
            "    self._exit_buffer()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 823, in _exit_buffer\n",
            "    self._check_buffer()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_vendor/rich/console.py\", line 2060, in _check_buffer\n",
            "    self.file.write(text)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import wandb\n",
        "\n",
        "# Login to Weights & Biases\n",
        "wandb.login(key=userdata.get('wandb'))\n",
        "\n",
        "# Login to Hugging Face\n",
        "hf_token = userdata.get('newhuggingface')\n",
        "!hf auth login --token {hf_token}"
      ],
      "metadata": {
        "id": "OzQ7gcC192YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "import json_repair\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "data_dir = \"/gdrive/MyDrive/llm-finetune\"\n",
        "base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "device = \"cuda\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "CM9uQWyE92aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wvo8k6gA92cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story = \"\"\"\n",
        "ذكرت مجلة فوربس أن العائلة تلعب دورا محوريا في تشكيل علاقة الأفراد بالمال،\n",
        " حيث تتأثر هذه العلاقة بأنماط السلوك المالي المتوارثة عبر الأجيال.\n",
        "\n",
        "التقرير الذي يستند إلى أبحاث الأستاذ الجامعي شاين إنيت حول\n",
        "الرفاه المالي يوضح أن لكل شخص \"شخصية مالية\" تتحدد وفقا لطريقة\n",
        " تفاعله مع المال، والتي تتأثر بشكل مباشر بتربية الأسرة وتجارب الطفولة.\n",
        "\n",
        " الأبعاد الثلاثة للعلاقة بالمال\n",
        "بحسب الدراسة، هناك ثلاثة أبعاد رئيسية تشكّل علاقتنا بالمال:\n",
        "\n",
        "الاكتساب (A): يميل الأفراد الذين ينتمون لهذا\n",
        " البعد إلى اعتبار المال سلعة قابلة للجمع، حيث يرون\n",
        "في تحقيق الثروة هدفا بحد ذاته. والجانب السلبي لهذا\n",
        " النمط هو إمكانية التحول إلى هوس بالثروة أو العكس،\n",
        " أي رفض تام لاكتساب المال باعتباره مصدرا للفساد.\n",
        "\n",
        "الاستخدام (U): يرى هؤلاء الأشخاص المال أداة للتمتع بالحياة، حيث يربطون قيمته بقدرته على توفير\n",
        "المتعة والراحة. ومع ذلك، قد يصبح\n",
        "البعض مدمنا على الإنفاق، في حين يتجه آخرون إلى التقشف المفرط خوفا من المستقبل.\n",
        "\n",
        "الإدارة (M): أصحاب هذا النمط يعتبرون المال مسؤولية تتطلب التخطيط الدقيق. لكن في بعض الحالات،\n",
        " قد يتحول الأمر إلى هوس مفرط بإدارة الإنفاق، مما يؤثر سلبا على العلاقات الشخصية.\n",
        "\n",
        " كيف تؤثر العائلة على علاقتنا بالمال؟\n",
        "يشير التقرير إلى أن التجارب الأسرية تلعب دورا رئيسيا في تحديد\n",
        " \"الشخصية المالية\" لكل فرد، على سبيل المثال، إذا كان أحد الوالدين يعتمد على المال\n",
        "كمكافأة للسلوك الجيد، فقد يتبنى الطفل لاحقا النمط نفسه في حياته البالغة.\n",
        "\n",
        "لتحليل هذه التأثيرات بشكل دقيق، طورت رابطة العلاج المالي\n",
        "(Financial Therapy Association) أداة تسمى مخطط الجينوم المالي (Money Genogram)،\n",
        "وهو نموذج يُستخدم لتحديد الأنماط المالية داخل العائلة.\n",
        "\n",
        "تتضمن هذه الأداة:\n",
        "\n",
        "رسم شجرة عائلية.\n",
        "تصنيف أفراد العائلة وفقا للأبعاد الثلاثة للعلاقة بالمال (A ،U ،M).\n",
        "تحديد ما إذا كان السلوك المالي لكل فرد صحيا (+) أو غير صحي (-).\n",
        "على سبيل المثال، إذا نشأ شخص في عائلة\n",
        "اعتادت على الإنفاق المفرط، فقد يكون لديه ميل قوي إلى اتباع النمط نفسه،\n",
        " أو العكس تماما، حيث يصبح مقتصدا بشكل مبالغ فيه كرد فعل نفسي.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9s7wECWP92eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StoryCategory = Literal[\n",
        "    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n",
        "    \"health\", \"entertainment\", \"science\",\n",
        "    \"not_specified\"\n",
        "]\n",
        "\n",
        "EntityType = Literal[\n",
        "    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"tie\",\n",
        "    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n",
        "]\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n",
        "    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n",
        "\n",
        "class NewsDetails(BaseModel):\n",
        "    story_title: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"A fully informative and SEO optimized title of the story.\")\n",
        "\n",
        "    story_keywords: List[str] = Field(..., min_items=1,\n",
        "                                      description=\"Relevant keywords associated with the story.\")\n",
        "\n",
        "    story_summary: List[str] = Field(\n",
        "                                    ..., min_items=1, max_items=5,\n",
        "                                    description=\"Summarized key points about the story (1-5 points).\"\n",
        "                                )\n",
        "\n",
        "    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n",
        "\n",
        "    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n",
        "                                        description=\"List of identified entities in the story.\")\n"
      ],
      "metadata": {
        "id": "QKpzWz_g92gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps(\n",
        "                NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "            ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Story Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "l6ut3kqJ92ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatedStory(BaseModel):\n",
        "    translated_title: str = Field(..., min_length=5, max_length=300,\n",
        "                                  description=\"Suggested translated title of the news story.\")\n",
        "    translated_content: str = Field(..., min_length=5,\n",
        "                                    description=\"Translated content of the news story.\")\n",
        "\n",
        "targeted_lang = \"English\"\n",
        "\n",
        "translation_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are a professional translator.\",\n",
        "            \"You will be provided by an Arabic text.\",\n",
        "            \"You have to translate the text into the `Targeted Language`.\",\n",
        "            \"Follow the provided Scheme to generate a JSON\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":  \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Targeted Language:\",\n",
        "            targeted_lang,\n",
        "            \"\",\n",
        "\n",
        "            \"## Translated Story:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "Zz3Xjp4292kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model =AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"auto\"\n",
        "\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "mFsT691E92mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "II0q303B92oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "_IRWZnmH92rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "tzA0xC-892st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_path = join(data_dir, \"news-sample.jsonl\")\n",
        "\n",
        "raw_data = []\n",
        "for line in open(raw_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    raw_data.append(\n",
        "        json.loads(line.strip())\n",
        "    )\n",
        "\n",
        "random.Random(101).shuffle(raw_data)\n",
        "print(f\"Raw data: {len(raw_data)}\")"
      ],
      "metadata": {
        "id": "vEpRKZ6i92vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data[0]['content']"
      ],
      "metadata": {
        "id": "eqid3Lt792wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_data_path = join(data_dir, \"sft.jsonl\")\n",
        "llm_finetunning_data = []\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "\n",
        "for line in open(sft_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    rec = json.loads(line.strip())\n",
        "\n",
        "    llm_finetunning_data.append({\n",
        "        \"system\": system_message,\n",
        "        \"instruction\": \"\\n\".join([\n",
        "            \"# Story:\",\n",
        "            rec[\"story\"],\n",
        "\n",
        "            \"# Task:\",\n",
        "            rec[\"task\"],\n",
        "\n",
        "            \"# Output Scheme:\",\n",
        "            rec[\"output_scheme\"],\n",
        "            \"\",\n",
        "\n",
        "            \"# Output JSON:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ]),\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"\\n\".join([\n",
        "            \"```json\",\n",
        "            json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "            \"```\"\n",
        "        ]),\n",
        "        \"history\": []\n",
        "    })\n",
        "\n",
        "random.Random(101).shuffle(llm_finetunning_data)"
      ],
      "metadata": {
        "id": "JDV1mpZn92yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(llm_finetunning_data)"
      ],
      "metadata": {
        "id": "47bxqyx_920l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = llm_finetunning_data[:2666]\n",
        "eval_ds = llm_finetunning_data[2666:]\n",
        "\n",
        "os.makedirs(join(data_dir,\"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(data_dir, \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(join(data_dir, \"llamafactory-finetune-data\", \"valdation.json\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ],
      "metadata": {
        "id": "yTNoq4Zz922r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(join(data_dir, \"llamafactory-finetune-data\", \"train.json\"))\n",
        "print(join(data_dir, \"llamafactory-finetune-data\", \"valdation.json\"))"
      ],
      "metadata": {
        "id": "tjP8tGjd924j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "rGpZKNHu927b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_target: all\n",
        "\n",
        "lora_rank: 16\n",
        "cutoff_len: 2048\n",
        "gradient_checkpointing: true\n",
        "\n",
        "\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "#resume_from_checkpoint: /gdrive/MyDrive/llm-finetuning/new-models/\n",
        "output_dir: /gdrive/MyDrive/llm-finetuning/new-models/\n",
        "logging_steps: 10\n",
        "save_steps: 100\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "hub_model_id: \"youssef224/analyzer\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n"
      ],
      "metadata": {
        "id": "gGhINeM992-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "yTaMkCU4-b-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # update /content/LLaMA-Factory/data/dataset_info.json and append\n",
        "# ```\n",
        "   \"news_finetune_train\": {\n",
        "        \"file_name\": \"/gdrive/MyDrive/llm-finetune/llamafactory-finetune-data/train.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    },\n",
        "    \"news_finetune_val\": {\n",
        "        \"file_name\": \"/gdrive/MyDrive/llm-finetune/llamafactory-finetune-data/valdation.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "hA1Fx3-Q-b7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uRBPfn7Y-b5g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}